{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from innvestigate.utils.keras import checks\n",
    "# from innvestigate.utils.keras import checks as kchecks\n",
    "# from innvestigate.utils.keras import backend as kb\n",
    "# from innvestigate.utils.keras import applications as kapp\n",
    "# from innvestigate import create_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_regression_feature_importance(model_file, feature_names):\n",
    "    \"\"\"\n",
    "    Extracts feature importance from a linear regression model stored in a .sav file.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_file (str): File path to the .sav file containing the linear regression model.\n",
    "    - feature_names (list): List of feature names.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing the feature names and their corresponding importance.\n",
    "    \"\"\"\n",
    "\n",
    "    model = joblib.load(model_file)\n",
    "\n",
    "    coefficients = model.coef_\n",
    "\n",
    "    absolute_coefficients = np.abs(coefficients)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    normalized_coefficients = scaler.fit_transform(absolute_coefficients.reshape(-1, 1)).flatten()\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': normalized_coefficients})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n",
    "def get_svm_feature_importance(model_file, feature_names, X_train=[]):\n",
    "    \"\"\"\n",
    "    Extracts feature importance from an SVM model stored in a .sav file.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_file (str): File path to the .sav file containing the SVM model.\n",
    "    - feature_names (list): List of feature names.\n",
    "    - X_train (DataFrame or array-like): Training data used to fit the SVM model.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing the feature names and their corresponding importance.\n",
    "    \"\"\"\n",
    "    model = joblib.load(model_file)\n",
    "\n",
    "    if model.kernel == 'linear':\n",
    "        coefficients = model.coef_.flatten()\n",
    "        importance = np.abs(coefficients)\n",
    "    else:\n",
    "        print(\"Feature importance for non-linear SVMs is not implemented yet. (Hard and Need to be done during the training processes)\")\n",
    "        return \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    normalized_importance = scaler.fit_transform(importance.reshape(-1, 1)).flatten()\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': normalized_importance})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n",
    "def get_lstm_feature_importance_from_file(model_file, feature_names, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Extracts feature importance from the weights of an LSTM model stored in a .pth.tar file.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_file (str): File path to the .pth.tar file containing the weights of the LSTM model.\n",
    "    - feature_names (list): List of feature names.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing the feature names and their corresponding importance.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(model_file, map_location=torch.device(device))\n",
    "    model_state_dict = state_dict['model']\n",
    "\n",
    "    hidden_size = 256\n",
    "    input_size = 1\n",
    "    num_layers = 4\n",
    "    lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    linear1 = nn.Linear(hidden_size, 64)\n",
    "    linear2 = nn.Linear(64, 1)\n",
    "\n",
    "    lstm_weight_ih = model_state_dict['layer1.weight_ih_l0'].T  # Shape: (hidden_size*4, input_size)\n",
    "    lstm_weight_hh = model_state_dict['layer1.weight_hh_l0'].T  # Shape: (hidden_size*4, hidden_size)\n",
    "    lstm_bias_ih = model_state_dict['layer1.bias_ih_l0']  # Shape: (hidden_size*4,)\n",
    "    lstm_bias_hh = model_state_dict['layer1.bias_hh_l0']  # Shape: (hidden_size*4,)\n",
    "    linear1_weight = model_state_dict['layer2.0.weight'].T  # Shape: (64, hidden_size)\n",
    "    linear1_bias = model_state_dict['layer2.0.bias']  # Shape: (64,)\n",
    "    linear2_weight = model_state_dict['layer3.weight'].T  # Shape: (1, 64)\n",
    "    linear2_bias = model_state_dict['layer3.bias']  # Shape: (1,)\n",
    "\n",
    "    importance1 = np.abs(np.matmul(lstm_weight_ih, np.diag(lstm_weight_hh.flatten())) + lstm_bias_ih + lstm_bias_hh)\n",
    "    importance2 = np.abs(np.matmul(linear1_weight, linear2_weight.flatten())) + linear1_bias + linear2_bias\n",
    "    importance = np.concatenate((importance1.flatten(), importance2))\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "\n",
    "def get_xgboost_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Extracts feature importance from an XGBoost model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained XGBoost model.\n",
    "    - feature_names (list): List of feature names.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing the feature names and their corresponding importance.\n",
    "    \"\"\"\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Importance\n",
      "3   Close    1.997547\n",
      "2     Low   -0.435193\n",
      "1    High   -0.457862\n",
      "0    Open   -0.531928\n",
      "4  Volume   -0.572564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivobook\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.4.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = \"models/LinearRegression.sav\"\n",
    "feature_names = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "feature_importance_df = get_linear_regression_feature_importance(path, feature_names)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Importance\n",
      "3   Close    1.182225\n",
      "1    High    0.615189\n",
      "0    Open    0.202764\n",
      "2     Low   -0.231189\n",
      "4  Volume   -1.768990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivobook\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.4.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = \"models/LinearRegression_log.sav\"\n",
    "feature_names = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "feature_importance_df = get_linear_regression_feature_importance(path, feature_names)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance for non-linear SVMs is not implemented yet. (Hard and Need to be done during the training processes)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivobook\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVR from version 1.4.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_file_path = 'models/SVM.sav'\n",
    "feature_names =  [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "X_train = [] \n",
    "feature_importance = get_svm_feature_importance(model_file_path, feature_names, X_train)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse an XGBoost model directly in the function \n",
    "# feature_importance = get_xgboost_feature_importance(xgb_model, feature_names)\n",
    "# print(\"Ranked Feature Importance:\")\n",
    "# print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 256. GiB for an array with shape (262144, 262144) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/LSTM1.pth.tar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m \u001b[43mget_lstm_feature_importance_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRanked Feature Importance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importance)\n",
      "Cell \u001b[1;32mIn[53], line 94\u001b[0m, in \u001b[0;36mget_lstm_feature_importance_from_file\u001b[1;34m(model_file, feature_names, device)\u001b[0m\n\u001b[0;32m     91\u001b[0m linear2_bias \u001b[38;5;241m=\u001b[39m model_state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer3.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Shape: (1,)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Feature importance calculation\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m importance1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mmatmul(lstm_weight_ih, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_weight_hh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m lstm_bias_ih \u001b[38;5;241m+\u001b[39m lstm_bias_hh)\n\u001b[0;32m     95\u001b[0m importance2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mmatmul(linear1_weight, linear2_weight\u001b[38;5;241m.\u001b[39mflatten())) \u001b[38;5;241m+\u001b[39m linear1_bias \u001b[38;5;241m+\u001b[39m linear2_bias\n\u001b[0;32m     96\u001b[0m importance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((importance1\u001b[38;5;241m.\u001b[39mflatten(), importance2))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\twodim_base.py:293\u001b[0m, in \u001b[0;36mdiag\u001b[1;34m(v, k)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    292\u001b[0m     n \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mabs\u001b[39m(k)\n\u001b[1;32m--> 293\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m         i \u001b[38;5;241m=\u001b[39m k\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 256. GiB for an array with shape (262144, 262144) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "model_file_path = 'models/LSTM1.pth.tar'\n",
    "feature_names = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "feature_importance = get_lstm_feature_importance_from_file(model_file_path, feature_names)\n",
    "print(\"Ranked Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
